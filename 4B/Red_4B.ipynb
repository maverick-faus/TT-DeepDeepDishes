{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Red_4B.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"ESm3A-yZxvZ8","colab_type":"text"},"cell_type":"markdown","source":["<img src=\"https://raw.githubusercontent.com/maverick-faus/Files/master/DeepDeamon2.png\" alt=\"drawing\" width=\"100px\"/>\n","# Red 4B - v1.2\n","TT: Deep Deep Dishes  \n","Food recommendation System  \n","ESCOM  \n","By Faus - 2K18\n"]},{"metadata":{"id":"gDELIWHHrCkf","colab_type":"code","outputId":"41111dfb-e247-4c82-abc3-fa425901e442","executionInfo":{"status":"ok","timestamp":1541570785694,"user_tz":360,"elapsed":5183,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"cell_type":"code","source":["!pip install -U tensorboardcolab\n","from tensorboardcolab import *"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorboardcolab\n","  Downloading https://files.pythonhosted.org/packages/73/3d/eaf745e162e471c5bb2737a407d8626fb8684a88cf085045456aeb841d3c/tensorboardcolab-0.0.19.tar.gz\n","Building wheels for collected packages: tensorboardcolab\n","  Running setup.py bdist_wheel for tensorboardcolab ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ab/74/02/cda602d1dc28b2f12eab313c49b9bfa14d6371326bc2590e06\n","Successfully built tensorboardcolab\n","Installing collected packages: tensorboardcolab\n","Successfully installed tensorboardcolab-0.0.19\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"xhQGOsjirDju","colab_type":"code","outputId":"418e3ce6-6479-4aaf-8255-43d3a4d7e8b5","executionInfo":{"status":"ok","timestamp":1541570793331,"user_tz":360,"elapsed":3903,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"cell_type":"code","source":["!pip install PyDrive\n","import os\n","from zipfile import ZipFile\n","from shutil import copy\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","from tensorboardcolab import *\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting PyDrive\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n","\u001b[K    100% |████████████████████████████████| 993kB 8.9MB/s \n","\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.4)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.2)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n","Building wheels for collected packages: PyDrive\n","  Running setup.py bdist_wheel for PyDrive ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n","Successfully built PyDrive\n","Installing collected packages: PyDrive\n","Successfully installed PyDrive-1.3.1\n"],"name":"stdout"}]},{"metadata":{"id":"ph7zUuJmrGU2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":360},"outputId":"eb3ca730-5c4e-4948-b80b-88621e0946c0","executionInfo":{"status":"ok","timestamp":1541570872075,"user_tz":360,"elapsed":59843,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":3,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package libfuse2:amd64.\n","(Reading database ... 22280 files and directories currently installed.)\n","Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package fuse.\n","Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n","Unpacking fuse (2.9.7-1ubuntu1) ...\n","Selecting previously unselected package google-drive-ocamlfuse.\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.0-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.0-0ubuntu1~ubuntu18.04.1) ...\n","Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n","Processing triggers for libc-bin (2.27-3ubuntu1) ...\n","Setting up fuse (2.9.7-1ubuntu1) ...\n","Setting up google-drive-ocamlfuse (0.7.0-0ubuntu1~ubuntu18.04.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"1DkAYus3yOYh","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gClpazXXyPJ6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"outputId":"5dd71a32-5a0d-4832-d0ad-97af3995075d","executionInfo":{"status":"ok","timestamp":1541570893231,"user_tz":360,"elapsed":3399,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}}},"cell_type":"code","source":["!ls drive"],"execution_count":5,"outputs":[{"output_type":"stream","text":["'Advanced Neural Networks'\n","'Artículos Varios TT'\n","'BBVA Hack - Chicos que lloran'\n","'Books Wishlist 2K18.ods'\n","'Books Wishlist 2K18.xlsx'\n","'Books Wishlist 2K18.xlsx.ods'\n"," Chatboot\n"," Classroom\n"," CodigoTT\n","'COMUNICADO Patish Complemento de Pago a sus proveedores.docx'\n","'COMUNICADO Patish Complemento de Pago a sus proveedores.odt'\n","'Copia de Red4B.ipynb'\n","'Curso DNN'\n","'Deep Daemon'\n"," deepdeep.backup\n","'Discurso excelencia.odt'\n","'Documento sin título.odt'\n","'Documentos Personales '\n","'Documento TT1'\n","'Documento TT1 (3a7c594f)'\n"," DriveC\n","'Hoja de cálculo sin título.ods'\n"," Imagenet_Hamburguer.ods\n"," IMG_2364Trim.mp4\n","'Prueba de Escritorio - DDD.ods'\n","'Publicidad dirigida mediante redes neuronales profundas.pdf'\n","'Retorno Inversiones Jose Shedid Merhy.xlsx'\n","'Retorno Inversiones Jose Shedid Merhy.xlsx.ods'\n","'Script Presentación TT1.odt'\n","'TT1 - Previos'\n"," TT2\n"," TT2_W2VReporteFinalFinal.docx\n"," Untitled0.ipynb\n"],"name":"stdout"}]},{"metadata":{"id":"aGNBeecCyUpP","colab_type":"code","colab":{}},"cell_type":"code","source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HDlsT_FSrKm1","colab_type":"code","colab":{}},"cell_type":"code","source":["fileIdLabelmap = '1lCUmg2TZsB_xduCL3ykBWWk8EitIDnTg'\n","#label map\n","downloaded1 = drive.CreateFile({'id': fileIdLabelmap})\n","downloaded1.GetContentFile(\"4B_train.pickle\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EWEWRU_nrfKt","colab_type":"code","outputId":"2fd6c1d9-f01d-4db5-b088-074c070a1fcc","executionInfo":{"status":"ok","timestamp":1541570919839,"user_tz":360,"elapsed":1604,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["4B_train.pickle  adc.json  drive  sample_data\n"],"name":"stdout"}]},{"metadata":{"id":"3OPpFKCcotKY","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pickle\n","from sklearn.metrics import confusion_matrix\n","#from tensorboardcolab import *"],"execution_count":0,"outputs":[]},{"metadata":{"id":"osT0Hrok5t9W","colab_type":"code","outputId":"756ebceb-2784-415c-b28b-ae7566561047","executionInfo":{"status":"ok","timestamp":1541570924749,"user_tz":360,"elapsed":265,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["print (tf.VERSION)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["1.12.0-rc2\n"],"name":"stdout"}]},{"metadata":{"id":"354T9z52otKc","colab_type":"code","colab":{}},"cell_type":"code","source":["def conv2d(x, W,name,padd,strid=[1,1,1,1]):\n","    #El stride de esa función no reduce el tamaño de la imagen\n","    return tf.nn.conv2d(x, W, strides=strid, padding=padd,name=name)\n","\n","def maxpool2d(x,ks,st):\n","    #           El st de esta función reduce la imagen a la mitad\n","    return tf.nn.max_pool(x, ksize=ks, strides=st, padding='SAME')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OvPU9TQ6otKd","colab_type":"code","colab":{}},"cell_type":"code","source":["def reset_graph():\n","    #Limpiamos la gráfic\n","    if 'sess' in globals() and sess:\n","        sess.close()\n","    tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YA6tbMxi1ah9","colab_type":"code","colab":{}},"cell_type":"code","source":["def deep_neural_convolutional_class(\n","    batch_size=20,\n","    image_size=[200,200],\n","    Drop_prob=1.0,\n","    learning_rate = 1e-3,\n","    n_nodes_hl0 = 2000,\n","    n_nodes_hl1 = 1000,\n","    n_nodes_hl2 = 500,\n","    n_nodes_hl3 = 100,\n","    n_classes=17\n","    ):\n","    \n","    reset_graph()\n","    #Place holder de entrada \n","    x= tf.placeholder(tf.float32,[batch_size,image_size[0],image_size[1],3], name='placeholder_img_entrada')\n","    y=tf.placeholder(tf.float32,name='placeholder_one_hot')\n","  \n","    #Diccionario de pesos convolucionales \n","    with tf.name_scope('pesos_bias') as scope1:\n","        weigths={\"w_conv1\":tf.Variable(tf.random_normal([5,5,3,32]),name='Pesos_1_32'),\n","                 \"w_conv2\":tf.Variable(tf.random_normal([5,5,32,64]),name='Pesos_1_64'),\n","                 \"w_conv3\":tf.Variable(tf.random_normal([3,3,64,128]),name='Pesos_1_128'),   \n","                 \"w_conv4\":tf.Variable(tf.random_normal([5,5,128,256]),name='Pesos_1_256'),\n","                 \"w_conv5\":tf.Variable(tf.random_normal([5,5,256,512]),name='Pesos_1_512'),\n","                 \"w_conv6\":tf.Variable(tf.random_normal([5,5,512,1024]),name='Pesos_1_1024'),\n","                }\n","        #Diccionario de bias\n","        biases={\"b_conv1\":tf.Variable(tf.random_normal([32]),name='Bias_1_32'),\n","                \"b_conv2\":tf.Variable(tf.random_normal([64]),name='Bias_1_64'),\n","                \"b_conv3\":tf.Variable(tf.random_normal([128]),name='Bias_1_128'),\n","                \"b_conv4\":tf.Variable(tf.random_normal([256]),name='Bias_1_256'),\n","                \"b_conv5\":tf.Variable(tf.random_normal([512]),name='Bias_1_512'),\n","                \"b_conv6\":tf.Variable(tf.random_normal([1024]),name='Bias_1_1024'),\n","               }\n","\n","    #Extractor de características\n","    with tf.name_scope('capas_conv') as scope2:\n","        conv1=tf.nn.relu(conv2d(x,weigths[\"w_conv1\"],'Capa_Conv_1','SAME')+biases[\"b_conv1\"],name='Func_relu_1')\n","        conv1=tf.nn.dropout(conv1,Drop_prob)\n","        conv1=maxpool2d(conv1,ks=[1,2,2,1],st=[1,2,2,1])\n","        #imagen resultante de 100x100x32\n","        print(conv1)\n","\n","        conv2=tf.nn.relu(conv2d(conv1,weigths[\"w_conv2\"],'Capa_Conv_2','SAME')+biases[\"b_conv2\"],name='Func_relu_2')\n","        conv2=tf.nn.dropout(conv2,Drop_prob)\n","        conv2=maxpool2d(conv2,ks=[1,2,2,1],st=[1,2,2,1])\n","        #imagen resultante de 50x50x64\n","        print(conv2)\n","\n","        conv3=tf.nn.relu(conv2d(conv2,weigths[\"w_conv3\"],'Capa_Conv_3','VALID')+biases[\"b_conv3\"],name='Func_relu_3')\n","        conv3=tf.nn.dropout(conv3,Drop_prob)\n","        conv3=maxpool2d(conv3,ks=[1,2,2,1],st=[1,2,2,1])\n","        #imagen resultante de 24x24x128\n","        print(conv3)\n","\n","        conv4=tf.nn.relu(conv2d(conv3,weigths[\"w_conv4\"],'Capa_Conv_4','SAME')+biases[\"b_conv4\"],name='Func_relu_4')\n","        conv4=tf.nn.dropout(conv4,Drop_prob)\n","        conv4=maxpool2d(conv4,ks=[1,2,2,1],st=[1,2,2,1])\n","        #imagen resultante de 12x12x256\n","        print(conv4)\n","\n","        conv5=tf.nn.relu(conv2d(conv4, weigths[\"w_conv5\"],'Capa_Conv_5','SAME')+biases[\"b_conv5\"],name='Func_relu_5')\n","        conv5=tf.nn.dropout(conv5,Drop_prob)\n","        conv5=maxpool2d(conv5,ks=[1,2,2,1],st=[1,2,2,1])\n","        #vector para clasificar de 6x6x512\n","        print(conv5)\n","\n","        #Embeding, son las caracteristicas fonales que se pasarán al MLP o red completamente conectada para clasifiacar\n","        embdeding=tf.reshape(conv5,[batch_size,6*6*512],name='Embeding')\n","        print(embdeding)\n","    \n","    #Red perceptron, declaración de capas, son diccionarios de pesos y bias.\n","    with tf.name_scope('capas_clasificador') as scope3:\n","        hidden_0_layer = {'weights':tf.Variable(tf.random_normal([6*6*512, n_nodes_hl0]),name='Capa_oculta_pesos_0'),\n","                          'biases':tf.Variable(tf.random_normal([n_nodes_hl0]),name='Capa_oculta_bias_0')}\n","\n","        hidden_1_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl0, n_nodes_hl1]),'Capa_oculta_pesos_1'),\n","                          'biases':tf.Variable(tf.random_normal([n_nodes_hl1]),name='Capa_oculta_bias_1')}\n","\n","        hidden_2_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2]),'Capa_oculta_pesos_2'),\n","                          'biases':tf.Variable(tf.random_normal([n_nodes_hl2]),name='Capa_oculta_bias_2')}\n","        \n","        hidden_3_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3]),'Capa_oculta_pesos_3'),\n","                          'biases':tf.Variable(tf.random_normal([n_nodes_hl3]),name='Capa_oculta_bias_3')}\n","\n","        output_layer = {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes]),'Capa_salida_pesos'),\n","                        'biases':tf.Variable(tf.random_normal([n_classes]),name='Capa_salida_bias'),}\n","    \n","    #W*P + B \n","    with tf.name_scope('op_clasificador') as scope4:\n","        \n","        l0 = tf.add(tf.matmul(embdeding,hidden_0_layer['weights'],name='Matmul_l0'), hidden_0_layer['biases'],name='Suma_Pesos_Bias_0')\n","        l0 = tf.nn.relu(l0,name='l0_relu_0')\n","\n","        l1 = tf.add(tf.matmul(l0,hidden_1_layer['weights'],name='Matmul_l1'), hidden_1_layer['biases'],name='Suma_Pesos_Bias_1')\n","        l1 = tf.nn.relu(l1,name='l1_relu_1')\n","\n","        l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights'],name='Matmul_l2'), hidden_2_layer['biases'],name='Suma_Pesos_Bias_2')\n","        l2 = tf.nn.relu(l2,name='l2_relu_2')\n","        \n","        l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights'],name='Matmul_l3'), hidden_3_layer['biases'],name='Suma_Pesos_Bias_3')\n","        l3 = tf.nn.relu(l3,name='l3_relu_3')\n","\n","        output = tf.matmul(l3,output_layer['weights'],name='Matmul_out') + output_layer['biases']\n","    \n","    \n","    \n","    \n","    \n","    # Declarando la funcion de costo y entrenamiento\n","    #Reduce mean, reduce la dimencion del tensor en un promedio es decir hace el promedio del costo o error\n","    y1,y2,y3,y4= tf.split(y,[5,5,4,3],1) #PaRTIR EL VECTOR EN CADA ONE HOT \n","    print(y1)\n","    print(y2)\n","    print(y3)\n","    print(y4)\n","    print(\"----\")\n","    output1,output2,output3,output4= tf.split(output, [5,5,4,3], 1)\n","    print(output)\n","    print(output1)\n","    print(output2)\n","    print(output3)\n","    print(output4)\n","    \n","    \n","    with tf.name_scope('costo_y_optimizador') as scope5:\n","        cost1 = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output1,labels=y1,name='softmax_cross_entropy_with_logits1'),name='reduce_mean1')\n","        cost2 = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output2,labels=y2,name='softmax_cross_entropy_with_logits2'),name='reduce_mean2')\n","        cost3 = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output3,labels=y3,name='softmax_cross_entropy_with_logits3'),name='reduce_mean3')\n","        cost4 = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=output4,labels=y4,name='softmax_cross_entropy_with_logits4'),name='reduce_mean4')\n","        cost = cost1+cost2+cost3+cost4\n","        optimizer = tf.train.AdamOptimizer(name='Adam',learning_rate=learning_rate).minimize(cost,name='minimo')\n","    \n","    \n","    tf.summary.scalar(\"costo\",cost)\n","    \n","    \n","    #correct = tf.equal(tf.argmax(output,1),tf.argmax(y,1),name='valores_correctos')\n","    #accuracy = tf.reduce_mean(tf.cast(correct,'float'),name='Promedio_exactitud') #porcentaje de error\n","\n","    \n","    correct_1 = tf.equal(tf.argmax(output1,1),tf.argmax(y1,1),name='valores_correctos1')\n","    accuracy_1 = tf.reduce_mean(tf.cast(correct_1,'float'),name='Promedio_exactitud1') #porcentaje de error\n","    \n","    correct_2 = tf.equal(tf.argmax(output2,1),tf.argmax(y2,1),name='valores_correctos2')\n","    accuracy_2 = tf.reduce_mean(tf.cast(correct_2,'float'),name='Promedio_exactitud2') #porcentaje de error\n","    \n","    correct_3 = tf.equal(tf.argmax(output3,1),tf.argmax(y3,1),name='valores_correctos3')\n","    accuracy_3 = tf.reduce_mean(tf.cast(correct_3,'float'),name='Promedio_exactitud3') #porcentaje de error\n","    \n","    correct_4 = tf.equal(tf.argmax(output4,1),tf.argmax(y4,1),name='valores_correctos4')\n","    accuracy_4 = tf.reduce_mean(tf.cast(correct_4,'float'),name='Promedio_exactitud4') #porcentaje de error\n","\n","    \n","    avg=accuracy_1*0.3+accuracy_2*0.3+accuracy_3*0.2+accuracy_4*0.2\n","    \n","    \n","    \n","    \n","    tf.summary.scalar(\"average\",avg)\n","    summaries = tf.summary.merge_all()\n","    \n","    return dict(\n","        x = x,\n","        y=y,\n","        embeding=conv5,\n","        output=output,\n","        saver = tf.train.Saver(),\n","        total_loss = cost,\n","        train_step = optimizer,\n","        summaries = summaries,\n","        average = avg,\n","        accuracy_precio = accuracy_1,\n","        accuracy_estilo = accuracy_2,\n","        accuracy_pan = accuracy_3,\n","        accuracy_side = accuracy_4,\n","        loss_precio = cost1,\n","        loss_estilo = cost2,\n","        loss_pan = cost3,\n","        loss_side = cost4\n","    )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S1D-HazpotKh","colab_type":"code","outputId":"075f0c75-a9f6-4a83-97bb-edb40b7aa003","executionInfo":{"status":"ok","timestamp":1541570938298,"user_tz":360,"elapsed":1824,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}},"colab":{"base_uri":"https://localhost:8080/","height":734}},"cell_type":"code","source":["deep_neural_convolutional_class()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Tensor(\"capas_conv/MaxPool:0\", shape=(20, 100, 100, 32), dtype=float32)\n","Tensor(\"capas_conv/MaxPool_1:0\", shape=(20, 50, 50, 64), dtype=float32)\n","Tensor(\"capas_conv/MaxPool_2:0\", shape=(20, 24, 24, 128), dtype=float32)\n","Tensor(\"capas_conv/MaxPool_3:0\", shape=(20, 12, 12, 256), dtype=float32)\n","Tensor(\"capas_conv/MaxPool_4:0\", shape=(20, 6, 6, 512), dtype=float32)\n","Tensor(\"capas_conv/Embeding:0\", shape=(20, 18432), dtype=float32)\n","Tensor(\"split:0\", dtype=float32)\n","Tensor(\"split:1\", dtype=float32)\n","Tensor(\"split:2\", dtype=float32)\n","Tensor(\"split:3\", dtype=float32)\n","----\n","Tensor(\"op_clasificador/add:0\", shape=(20, 17), dtype=float32)\n","Tensor(\"split_1:0\", shape=(20, 5), dtype=float32)\n","Tensor(\"split_1:1\", shape=(20, 5), dtype=float32)\n","Tensor(\"split_1:2\", shape=(20, 4), dtype=float32)\n","Tensor(\"split_1:3\", shape=(20, 3), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-13-0561348192b8>:133: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'accuracy_estilo': <tf.Tensor 'Promedio_exactitud2:0' shape=() dtype=float32>,\n"," 'accuracy_pan': <tf.Tensor 'Promedio_exactitud3:0' shape=() dtype=float32>,\n"," 'accuracy_precio': <tf.Tensor 'Promedio_exactitud1:0' shape=() dtype=float32>,\n"," 'accuracy_side': <tf.Tensor 'Promedio_exactitud4:0' shape=() dtype=float32>,\n"," 'average': <tf.Tensor 'add_2:0' shape=() dtype=float32>,\n"," 'embeding': <tf.Tensor 'capas_conv/MaxPool_4:0' shape=(20, 6, 6, 512) dtype=float32>,\n"," 'loss_estilo': <tf.Tensor 'costo_y_optimizador/reduce_mean2:0' shape=() dtype=float32>,\n"," 'loss_pan': <tf.Tensor 'costo_y_optimizador/reduce_mean3:0' shape=() dtype=float32>,\n"," 'loss_precio': <tf.Tensor 'costo_y_optimizador/reduce_mean1:0' shape=() dtype=float32>,\n"," 'loss_side': <tf.Tensor 'costo_y_optimizador/reduce_mean4:0' shape=() dtype=float32>,\n"," 'output': <tf.Tensor 'op_clasificador/add:0' shape=(20, 17) dtype=float32>,\n"," 'saver': <tensorflow.python.training.saver.Saver at 0x7f2b915bc0f0>,\n"," 'summaries': <tf.Tensor 'Merge/MergeSummary:0' shape=() dtype=string>,\n"," 'total_loss': <tf.Tensor 'costo_y_optimizador/add_2:0' shape=() dtype=float32>,\n"," 'train_step': <tf.Operation 'costo_y_optimizador/minimo' type=NoOp>,\n"," 'x': <tf.Tensor 'placeholder_img_entrada:0' shape=(20, 200, 200, 3) dtype=float32>,\n"," 'y': <tf.Tensor 'placeholder_one_hot:0' shape=<unknown> dtype=float32>}"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"G52X0mlOotKl","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_img_encoder(G,nameBatch,numbatch, num_epochs,learning_rate = 1e-2, \n","                      num_steps = 15, batch_size = 20, verbose = True, save=False,checkpoint=False):\n","    #tf.set_random_seed(2345)\n","    print('Start training')\n","    total_loss = 0\n","    with tf.Session() as sess:\n","        \n","        writer = tf.summary.FileWriter(\"./CNNClass\")\n","        tf.summary.FileWriter.add_graph(writer,sess.graph)\n","        \n","        sess.run(tf.global_variables_initializer()) ###########\n","        print('Enter here3')\n","        if checkpoint:\n","            ENCname=\"./\"+checkpoint+\".ckpt\"\n","            G['saver'].restore(sess, ENCname)\n","        training_losses = []\n","        for epoch in range(num_epochs):\n","            epoch_loss = 0\n","\n","            for key in range(numbatch):\n","                data=np.array(pickle.load(open(nameBatch+\".pickle\",\"rb\")))\n","                dim,var=data.shape\n","                for j in range(int(dim/batch_size)): #Unir cada etiqueta que viene por separado a un vector de 17 posiciones\n","                    epoch_x=data[batch_size*(j):batch_size*(j+1),0].tolist()\n","                    epoch_Y1=data[batch_size*(j):batch_size*(j+1),1].tolist()\n","                    epoch_Y2=data[batch_size*(j):batch_size*(j+1),2].tolist()\n","                    epoch_Y3=data[batch_size*(j):batch_size*(j+1),3].tolist()\n","                    epoch_Y4=data[batch_size*(j):batch_size*(j+1),4].tolist()\n","                    new_Y =[]\n","                    for k in range(len(epoch_Y1)):\n","                      new_Y.append(epoch_Y1[k]+epoch_Y2[k]+epoch_Y3[k]+epoch_Y4[k])\n","\n","                    feed_dict={G['x']: epoch_x,\n","                               G['y']: new_Y,\n","                              }\n","                \n","                    total_loss,_,summ,avg,a_precio,a_estilo,a_pan,a_side,l_precio,l_estilo,l_pan,l_side = sess.run([G[\"total_loss\"],\n","                                                                                                                         G[\"train_step\"],\n","                                                                                                                         G[\"summaries\"],\n","                                                                                                                         G[\"average\"],\n","                                                                                                                         G[\"accuracy_precio\"],\n","                                                                                                                         G[\"accuracy_estilo\"],\n","                                                                                                                         G[\"accuracy_pan\"],\n","                                                                                                                         G[\"accuracy_side\"],\n","                                                                                                                         G[\"loss_precio\"],\n","                                                                                                                         G[\"loss_estilo\"],\n","                                                                                                                         G[\"loss_pan\"],\n","                                                                                                                         G[\"loss_side\"]],feed_dict)\n","\n","                epoch_loss += total_loss\n","                writer.add_summary(summ,epoch)\n","                \n","                tbc.save_value(\"Evaluación de recomendación\", \"average\", epoch, avg)\n","                tbc.save_value(\"Accuracy/Precio\", \"accuracyP\", epoch, a_precio)\n","                tbc.save_value(\"Accuracy/Estilo\", \"accuracyE\", epoch, a_estilo)\n","                tbc.save_value(\"Accuracy/Pan\", \"accuracyB\", epoch, a_pan)\n","                tbc.save_value(\"Accuracy/Guarnición\", \"accuracyS\", epoch, a_side)\n","                \n","                tbc.save_value(\"Loss\", \"loss\", epoch, epoch_loss)\n","                tbc.save_value(\"Loss/Precio\", \"lossP\", epoch, l_precio)\n","                tbc.save_value(\"Loss/Estilo\", \"lossE\", epoch, l_estilo)\n","                tbc.save_value(\"Loss/Pan\", \"lossB\", epoch, l_pan)\n","                tbc.save_value(\"Loss/Guarnición\", \"lossS\", epoch, l_side)\n","                \n","                tbc.flush_line(\"loss\")\n","                tbc.flush_line(\"lossP\")\n","                tbc.flush_line(\"lossE\")\n","                tbc.flush_line(\"lossB\")\n","                tbc.flush_line(\"lossS\")\n","                \n","                tbc.flush_line(\"average\")\n","                tbc.flush_line(\"accuracyP\")\n","                tbc.flush_line(\"accuracyE\")\n","                tbc.flush_line(\"accuracyB\")\n","                tbc.flush_line(\"accuracyS\")               \n","                \n","            if verbose:\n","                print(\"+------------------+\\n|                  |\\n|      Epoch:      |\\n|      \",epoch,\"         |\\n|                  |\\n|      Loss:       |\\n|  %e    |\\n|                  |\\n+------------------+\\n (\\__/) | |\\n (●ㅅ●) | |  \\n /    っ    \\n\"%(epoch_loss))\n","                #print(\"Epoch\", epoch, \"  Loss:\", epoch_loss, \"  Accurracy:\", accuracy)\n","            training_losses.append(epoch_loss)\n","\n","        if isinstance(save, str):\n","            ENCname=\"drive/CodigoTT/4B/ckpt4B/\"+save+\".ckpt\"\n","            G['saver'].save(sess, ENCname)\n","    return training_losses"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bUM1uBnCgmNI","colab_type":"code","outputId":"c476eae9-7cd2-4c51-f4b2-6be90b55ef11","executionInfo":{"status":"ok","timestamp":1541571908273,"user_tz":360,"elapsed":12272,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["tbc=TensorBoardColab()"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Wait for 8 seconds...\n","TensorBoard link:\n","http://98b4c13e.ngrok.io\n"],"name":"stdout"}]},{"metadata":{"id":"TdXT4U0botKn","colab_type":"code","outputId":"5127ff8d-f3db-4f0b-df87-013dc5ebf5a1","executionInfo":{"status":"ok","timestamp":1541572854340,"user_tz":360,"elapsed":945995,"user":{"displayName":"José Faustinos","photoUrl":"https://lh3.googleusercontent.com/-a6P7G5s5oIY/AAAAAAAAAAI/AAAAAAAAAI8/YHIp_dB4VOQ/s64/photo.jpg","userId":"04841510125594790710"}},"colab":{"base_uri":"https://localhost:8080/","height":9945}},"cell_type":"code","source":["path = '4B_train' \n","encoder=deep_neural_convolutional_class(batch_size = 20)\n","train_img_encoder(encoder,path,1,num_epochs=40,save=\"4B3\",batch_size = 20,verbose = True)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Tensor(\"capas_conv/MaxPool:0\", shape=(20, 100, 100, 32), dtype=float32)\n","Tensor(\"capas_conv/MaxPool_1:0\", shape=(20, 50, 50, 64), dtype=float32)\n","Tensor(\"capas_conv/MaxPool_2:0\", shape=(20, 24, 24, 128), dtype=float32)\n","Tensor(\"capas_conv/MaxPool_3:0\", shape=(20, 12, 12, 256), dtype=float32)\n","Tensor(\"capas_conv/MaxPool_4:0\", shape=(20, 6, 6, 512), dtype=float32)\n","Tensor(\"capas_conv/Embeding:0\", shape=(20, 18432), dtype=float32)\n","Tensor(\"split:0\", dtype=float32)\n","Tensor(\"split:1\", dtype=float32)\n","Tensor(\"split:2\", dtype=float32)\n","Tensor(\"split:3\", dtype=float32)\n","----\n","Tensor(\"op_clasificador/add:0\", shape=(20, 17), dtype=float32)\n","Tensor(\"split_1:0\", shape=(20, 5), dtype=float32)\n","Tensor(\"split_1:1\", shape=(20, 5), dtype=float32)\n","Tensor(\"split_1:2\", shape=(20, 4), dtype=float32)\n","Tensor(\"split_1:3\", shape=(20, 3), dtype=float32)\n","Start training\n","Enter here3\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       0          |\n","|                  |\n","|      Loss:       |\n","|  1.026257e+15    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       1          |\n","|                  |\n","|      Loss:       |\n","|  4.694089e+14    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       2          |\n","|                  |\n","|      Loss:       |\n","|  3.649552e+14    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       3          |\n","|                  |\n","|      Loss:       |\n","|  2.435841e+14    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       4          |\n","|                  |\n","|      Loss:       |\n","|  1.735444e+14    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       5          |\n","|                  |\n","|      Loss:       |\n","|  1.637814e+14    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       6          |\n","|                  |\n","|      Loss:       |\n","|  1.226577e+14    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       7          |\n","|                  |\n","|      Loss:       |\n","|  9.547862e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       8          |\n","|                  |\n","|      Loss:       |\n","|  1.100598e+14    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       9          |\n","|                  |\n","|      Loss:       |\n","|  8.673052e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       10          |\n","|                  |\n","|      Loss:       |\n","|  4.382504e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       11          |\n","|                  |\n","|      Loss:       |\n","|  3.068450e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       12          |\n","|                  |\n","|      Loss:       |\n","|  2.411074e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       13          |\n","|                  |\n","|      Loss:       |\n","|  3.725560e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       14          |\n","|                  |\n","|      Loss:       |\n","|  4.301918e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       15          |\n","|                  |\n","|      Loss:       |\n","|  4.118607e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       16          |\n","|                  |\n","|      Loss:       |\n","|  2.087271e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       17          |\n","|                  |\n","|      Loss:       |\n","|  1.871104e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       18          |\n","|                  |\n","|      Loss:       |\n","|  7.348003e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       19          |\n","|                  |\n","|      Loss:       |\n","|  2.501706e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       20          |\n","|                  |\n","|      Loss:       |\n","|  2.835881e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       21          |\n","|                  |\n","|      Loss:       |\n","|  1.179949e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       22          |\n","|                  |\n","|      Loss:       |\n","|  1.462132e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       23          |\n","|                  |\n","|      Loss:       |\n","|  7.903244e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       24          |\n","|                  |\n","|      Loss:       |\n","|  1.064726e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       25          |\n","|                  |\n","|      Loss:       |\n","|  4.316367e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       26          |\n","|                  |\n","|      Loss:       |\n","|  6.719044e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       27          |\n","|                  |\n","|      Loss:       |\n","|  8.183345e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       28          |\n","|                  |\n","|      Loss:       |\n","|  1.413876e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       29          |\n","|                  |\n","|      Loss:       |\n","|  6.744672e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       30          |\n","|                  |\n","|      Loss:       |\n","|  2.015744e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       31          |\n","|                  |\n","|      Loss:       |\n","|  2.669813e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       32          |\n","|                  |\n","|      Loss:       |\n","|  6.976966e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       33          |\n","|                  |\n","|      Loss:       |\n","|  1.386376e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       34          |\n","|                  |\n","|      Loss:       |\n","|  5.753074e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       35          |\n","|                  |\n","|      Loss:       |\n","|  1.458986e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       36          |\n","|                  |\n","|      Loss:       |\n","|  1.154343e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       37          |\n","|                  |\n","|      Loss:       |\n","|  5.869966e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       38          |\n","|                  |\n","|      Loss:       |\n","|  7.773432e+12    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","+------------------+\n","|                  |\n","|      Epoch:      |\n","|       39          |\n","|                  |\n","|      Loss:       |\n","|  1.206234e+13    |\n","|                  |\n","+------------------+\n"," (\\__/) | |\n"," (●ㅅ●) | |  \n"," /    っ    \n","\n","WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n","Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n","'Capa_oculta_pesos_1' has type str, but expected one of: int, long, bool\n","WARNING:tensorflow:Issue encountered when serializing variables.\n","Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n","'Capa_oculta_pesos_1' has type str, but expected one of: int, long, bool\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[1026256934010880.0,\n"," 469408887603200.0,\n"," 364955215855616.0,\n"," 243584070582272.0,\n"," 173544361164800.0,\n"," 163781430738944.0,\n"," 122657655750656.0,\n"," 95478624550912.0,\n"," 110059778473984.0,\n"," 86730522755072.0,\n"," 43825036787712.0,\n"," 30684496265216.0,\n"," 24110736343040.0,\n"," 37255603290112.0,\n"," 43019176771584.0,\n"," 41186068791296.0,\n"," 20872706392064.0,\n"," 18711035510784.0,\n"," 7348003274752.0,\n"," 25017060425728.0,\n"," 28358811320320.0,\n"," 11799491510272.0,\n"," 14621319626752.0,\n"," 7903244189696.0,\n"," 10647260626944.0,\n"," 4316366897152.0,\n"," 6719044321280.0,\n"," 8183345053696.0,\n"," 14138760757248.0,\n"," 6744671518720.0,\n"," 2015743967232.0,\n"," 2669812908032.0,\n"," 6976965705728.0,\n"," 1386376331264.0,\n"," 5753074089984.0,\n"," 14589863395328.0,\n"," 11543428202496.0,\n"," 5869965672448.0,\n"," 7773431529472.0,\n"," 12062337007616.0]"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"diCsHyAWOBbi","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WvpWJ7Z9A1Lq","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}